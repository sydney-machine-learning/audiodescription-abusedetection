{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "599a1e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/eye4got/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s - %(asctime)s - %(message)s')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pydub import AudioSegment\n",
    "from pyannote.audio import Pipeline\n",
    "import json\n",
    "\n",
    "from silero_vad import read_audio, get_speech_timestamps, load_silero_vad\n",
    "\n",
    "with open('config.json') as fileobj:\n",
    "    hf_token = json.load(fileobj)['hugging_face_token']\n",
    "\n",
    "pyannote_model = 'pyannote/speaker-diarization-3.1'\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import os\n",
    "\n",
    "import utils\n",
    "from itertools import chain\n",
    "import gc\n",
    "\n",
    "import tiktoken\n",
    "import whisper\n",
    "whisper_tokenizer = whisper.tokenizer.get_tokenizer(tiktoken.get_encoding(tiktoken.list_encoding_names()[-1]), num_languages=1)\n",
    "\n",
    "whisper_model = 'turbo'\n",
    "silero_threshold = 0.5\n",
    "whisper_beam = 3\n",
    "whisper_ns_prob = 0.2\n",
    "\n",
    "import data_extraction as da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47a5cee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: upload files to Kaggle?\n",
    "# Raw files vs transcript (as one parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3786e9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "films_list_df = da.get_or_create_subtitles_data(os.path.join(da.sub_dir, 'movie_index.parquet'), da.sub_dir)\n",
    "\n",
    "# TODO: add download scripts for transcript downloads\n",
    "\n",
    "# zenodo_get.download(\n",
    "#     record_or_doi=4881008,\n",
    "#     output_dir=os.path.join('data')\n",
    "# )\n",
    "\n",
    "# Unzip zip files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2783aa1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2025-05-29 12:53:28,214 - Applying Silero VAD to Annie Hall\n",
      "INFO - 2025-05-29 12:54:13,512 - Started pyannote pipeline for Annie Hall\n",
      "INFO - 2025-05-29 12:54:13,946 - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]\n",
      "INFO - 2025-05-29 12:54:13,947 - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "/home/eye4got/projects/honours-thesis/.venv/lib/python3.11/site-packages/pyannote/audio/utils/reproducibility.py:74: ReproducibilityWarning: TensorFloat-32 (TF32) has been disabled as it might lead to reproducibility issues and lower accuracy.\n",
      "It can be re-enabled by calling\n",
      "   >>> import torch\n",
      "   >>> torch.backends.cuda.matmul.allow_tf32 = True\n",
      "   >>> torch.backends.cudnn.allow_tf32 = True\n",
      "See https://github.com/pyannote/pyannote-audio/issues/1370 for more details.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/eye4got/projects/honours-thesis/.venv/lib/python3.11/site-packages/pyannote/audio/models/blocks/pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1839.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n"
     ]
    }
   ],
   "source": [
    "# Torch (pyannote) isn't familiar with MP3 files, so convert to wav for effective performance\n",
    "# Perform diarization to help separate narration in audio description from dialogue in original movie\n",
    "# Finally use OpenAI's Whisper to convert to a transcript\n",
    "\n",
    "whisper_sr = 16000\n",
    "mp3_files = [x for x in os.listdir(da.trans_mp3_dir) if os.path.splitext(x)[-1].lower() == '.mp3']\n",
    "mp3_files = ['Annie Hall.mp3', 'The Silence Of The Lambs.mp3', 'The Departed.mp3'] + mp3_files\n",
    "\n",
    "for mp3_filename in mp3_files:\n",
    "    movie_name = utils.remove_ext(mp3_filename)\n",
    "    \n",
    "    if movie_name in ('Oppenheimer', 'Killers Of The Flower Moon', 'Guardians of the Galaxy Vol. 3', 'Elvis', 'All Quiet on the Western Front', 'Avatar The Way of Water'):\n",
    "        continue\n",
    "    \n",
    "    vad_df_path = os.path.join(da.voice_activity_dir, f'{movie_name}-vad.parquet')\n",
    "    seg_df_path = os.path.join(da.diarization_dir, f'{movie_name}-diarization.parquet')\n",
    "    curr_transcript_fp = os.path.join(da.transcription_dir, da.transcript_df_fp.format(movie_name=movie_name))\n",
    "    wav_filepath = os.path.join(da.trans_mp3_dir, f'{movie_name}_speech_only.wav')\n",
    "    \n",
    "    if not os.path.exists(curr_transcript_fp) or not os.path.exists(seg_df_path):\n",
    "        \n",
    "        logging.info(f'Applying Silero VAD to {movie_name}')\n",
    "        silero_model = load_silero_vad()\n",
    "        \n",
    "        full_silero_audio = read_audio(os.path.join(da.trans_mp3_dir, mp3_filename))\n",
    "        speech_timestamps = get_speech_timestamps(full_silero_audio, silero_model, threshold=silero_threshold, speech_pad_ms=200)\n",
    "        pd.DataFrame(speech_timestamps).to_parquet(vad_df_path)\n",
    "        utils.cleanup_model(silero_model)\n",
    "\n",
    "        # Now cut audio down to just dialogue\n",
    "        full_audio = AudioSegment.from_mp3(os.path.join(da.trans_mp3_dir, mp3_filename))\n",
    "        dialogue_only_audio = AudioSegment.empty()\n",
    "\n",
    "        for seg in speech_timestamps:\n",
    "            dialogue_only_audio += full_audio[seg['start']:seg['end']]\n",
    "            \n",
    "        dialogue_only_audio.export(wav_filepath, format=\"wav\")\n",
    "        \n",
    "    # Only perform diarization if parquet of dialogue doesn't exist\n",
    "    if not os.path.exists(seg_df_path):\n",
    "        logging.info(f'Started pyannote pipeline for {movie_name}')\n",
    "        pyannote_pipeline = Pipeline.from_pretrained(pyannote_model, use_auth_token=hf_token)\n",
    "        pyannote_pipeline.to(device)\n",
    "        \n",
    "        dz = pyannote_pipeline({'audio': wav_filepath})\n",
    "        \n",
    "        # Extract start and end times from segments object and split integer out from 'SPEAKER_x' labels\n",
    "        records = [(x[0].start, x[0].end, int(x[2].split('_')[-1])) for x in dz.itertracks(yield_label = True)]\n",
    "        segments_df = pd.DataFrame(records, columns=['start', 'end', 'speaker'])\n",
    "        \n",
    "        agg_seg_df = da.aggregate_segments(segments_df)\n",
    "        \n",
    "        # Assume narrator speaks first (describing opening logos etc)\n",
    "        narrator_id = agg_seg_df['speaker'].iloc[0]\n",
    "        agg_seg_df['is_dialogue'] = agg_seg_df['speaker'].ne(narrator_id)\n",
    "        agg_seg_df['movie_name'] = movie_name\n",
    "        \n",
    "        agg_seg_df['start_frame'] = (whisper_sr * agg_seg_df['start']).astype(int)\n",
    "        agg_seg_df['end_frame'] = (whisper_sr * agg_seg_df['end']).astype(int)\n",
    "        \n",
    "        agg_seg_df.to_parquet(seg_df_path)\n",
    "        \n",
    "        utils.cleanup_model(pyannote_pipeline)\n",
    "        del dz\n",
    "        \n",
    "    segments_df = pd.read_parquet(seg_df_path)\n",
    "    narrator_df = segments_df[~segments_df.is_dialogue].copy()\n",
    "        \n",
    "    if not os.path.exists(curr_transcript_fp):\n",
    "        model = whisper.load_model(whisper_model, device=device)\n",
    "        audio = whisper.load_audio(wav_filepath)\n",
    "        seg_start_arr, seg_end_arr = narrator_df['start_frame'].values, narrator_df['end_frame'].values\n",
    "        \n",
    "        segment_list = []\n",
    "\n",
    "        for ii in range(len(seg_start_arr)):\n",
    "            if ii % 50 == 0:\n",
    "                logging.info(f'{movie_name} Segment: {ii + 1} / {len(seg_start_arr)}')\n",
    "            segment = audio[seg_start_arr[ii]: seg_end_arr[ii]]\n",
    "            \n",
    "            segment_list.append(model.transcribe(segment, language='en', beam_size=whisper_beam, no_speech_threshold=whisper_ns_prob, condition_on_previous_text=False)) \n",
    "        \n",
    "        narrator_df['text'] = [x['text'] for x in segment_list]\n",
    "        narrator_df.to_parquet(curr_transcript_fp)\n",
    "        \n",
    "        utils.cleanup_model(model)\n",
    "        del audio\n",
    "        \n",
    "    # Delete Wav File afterwards as they are quick to generate and consume too much space\n",
    "    if os.path.exists(wav_filepath):\n",
    "        os.remove(wav_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e0d983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup Missed wav files\n",
    "wav_files = [x for x in os.listdir(da.trans_mp3_dir) if os.path.splitext(x)[-1].lower() == '.wav']\n",
    "\n",
    "for path in wav_files:\n",
    "    os.remove(os.path.join(da.trans_mp3_dir, path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fb1c10",
   "metadata": {},
   "source": [
    "### Subtitles Editted\n",
    "\n",
    "File Completely Empty: X-Men, Finding Neverland, Mr Mrs Smith\n",
    "Grease: Line 6916\n",
    "Hangover Part II: Timestamps messed up line 5578\n",
    "Super Mario Bros. Movie: Line 3877, Missing hours \n",
    "The Social Network: Counter 658, 1507, 1526\n",
    "\n",
    "Index Titles Edited:\n",
    "- Goodbye Columbus\n",
    "- Monsters Inc\n",
    "- What's Up, Doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c826069",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_subs_df_list = []\n",
    "\n",
    "for movie_cat in ('Blockbusters', 'Oscar'):\n",
    "    cat_mask = films_list_df.fame_category.eq(movie_cat)\n",
    "    for year in films_list_df.year.unique():\n",
    "        year_dir = os.path.join(da.sub_by_year_dir, movie_cat, str(year))\n",
    "        for movie_fp in os.listdir(year_dir):\n",
    "            full_subs_df_list.append(da.extract_single_subs_file(os.path.join(year_dir, movie_fp)))\n",
    "                \n",
    "full_subs_df = pd.concat(full_subs_df_list)\n",
    "\n",
    "full_subs_df['movie'] = full_subs_df['movie'].str.strip().str.replace('-', ' ')\n",
    "films_list_df['movie'] = films_list_df['movie'].str.strip().str.replace('-', ' ').str.replace(\"'\", ' ').str.replace('&', 'and')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9635c473",
   "metadata": {},
   "outputs": [],
   "source": [
    "films_list_df.loc[films_list_df.movie.eq('Don t Look Up'), 'movie'] = 'Dont Look Up'\n",
    "films_list_df.loc[films_list_df.movie.eq('Goodbye,Columbus'), 'movie'] = 'Goodbye Columbus'\n",
    "films_list_df.loc[films_list_df.movie.eq('Summer of  42'), 'movie'] = 'Summer of 42'\n",
    "films_list_df.loc[films_list_df.movie.eq('What s Up, Doc_'), 'movie'] = 'What s Up, Doc'\n",
    "films_list_df.loc[films_list_df.movie.eq('Monsters, Inc.'), 'movie'] = 'Monsters Inc'\n",
    "\n",
    "combined_subs_df = full_subs_df.merge(films_list_df, how='left')\n",
    "combined_subs_df.to_parquet(da.sub_df_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416dbcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_transcripts_df_list = []\n",
    "longitudinal_movies = [utils.remove_ext(x) for x in os.listdir(da.trans_mp3_dir)]\n",
    "\n",
    "for filename in os.listdir(da.transcription_dir):\n",
    "    if filename.split('-')[0] in longitudinal_movies:\n",
    "        all_transcripts_df_list.append(pd.read_parquet(os.path.join(da.transcription_dir, filename)))\n",
    "\n",
    "all_transcripts_df = pd.concat(all_transcripts_df_list)\n",
    "all_transcripts_df.to_parquet(da.all_transcripts_df_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
