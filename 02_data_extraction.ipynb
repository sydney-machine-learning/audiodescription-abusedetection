{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599a1e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2025-06-10 12:20:10,702 - PyTorch version 2.7.0 available.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/eye4got/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "INFO - 2025-06-10 12:20:17,777 - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]\n",
      "INFO - 2025-06-10 12:20:17,777 - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s - %(asctime)s - %(message)s')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pydub import AudioSegment\n",
    "\n",
    "from evaluate import load\n",
    "\n",
    "import os\n",
    "\n",
    "pyannote_model = 'pyannote/speaker-diarization-3.1'\n",
    "embedding_model = \"pyannote/embedding\" # speechbrain/spkrec-ecapa-voxceleb\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "use_vad = True\n",
    "narr_cosine_sim_lim = 0.14\n",
    "diag_cosine_sim_lim = 1\n",
    "\n",
    "whisper_model = 'turbo'\n",
    "silero_threshold = 0.5\n",
    "\n",
    "whisper_config = {\n",
    "    'beam_size': 1,\n",
    "    'no_speech_threshold': 0.1,\n",
    "    'condition_on_previous_text': False\n",
    "}\n",
    "\n",
    "import data_extraction as da\n",
    "import stt\n",
    "import utils\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "logging.getLogger(\"speechbrain\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"pyannote\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3786e9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "films_list_df = da.get_or_create_subtitles_data(os.path.join(da.sub_dir, 'movie_index.parquet'), da.sub_dir)\n",
    "\n",
    "# TODO: add download scripts for transcript downloads\n",
    "\n",
    "# zenodo_get.download(\n",
    "#     record_or_doi=4881008,\n",
    "#     output_dir=os.path.join('data')\n",
    "# )\n",
    "\n",
    "# Unzip zip files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "669822e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(da.transcription_dir, 'manual', 'Annie Hall.txt')) as fileobj:\n",
    "    raw_annie_hall_man_txt = fileobj.read()\n",
    "annie_hall_man_txt = raw_annie_hall_man_txt.replace('\\n', ' ')\n",
    "\n",
    "def calc_cer_wer(movie_name: str, ref_txt: str):\n",
    "    trans_df = pd.read_parquet(os.path.join(da.transcription_dir, da.transcript_df_fp.format(movie_name=movie_name)))\n",
    "    trans_df = trans_df[trans_df['text'].ne(' Thank you.')]['text']\n",
    "    trans_txt = ''.join(trans_df.str.replace('[\\.,\"\\?]', '', regex=True)).lower().replace('-', ' ')\n",
    "    \n",
    "    cer, wer = load(\"cer\"), load(\"wer\")\n",
    "    cer_score = cer.compute(predictions=[trans_txt], references=[ref_txt])\n",
    "    wer_score = wer.compute(predictions=[trans_txt], references=[ref_txt])\n",
    "    \n",
    "    return cer_score, wer_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7381f340",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2025-06-10 12:20:17,879 - Applying Silero VAD to Mission Impossible - Dead Reckoning Part One\n",
      "INFO - 2025-06-10 12:21:19,747 - Slicing up audio from Mission Impossible - Dead Reckoning Part One to speech only\n",
      "INFO - 2025-06-10 12:22:10,234 - Started pyannote pipeline for Mission Impossible - Dead Reckoning Part One\n",
      "INFO - 2025-06-10 12:33:50,925 - Lightning automatically upgraded your loaded checkpoint from v1.2.7 to v2.5.0.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/torch/pyannote/models--pyannote--embedding/snapshots/4db4899737a38b2d618bbd74350915aa10293cb2/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.8.1+cu102, yours is 2.7.0+cu126. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2025-06-10 12:34:23,797 - Segment: 1 / 329\n",
      "INFO - 2025-06-10 12:35:43,948 - Segment: 51 / 329\n",
      "INFO - 2025-06-10 12:36:50,401 - Segment: 101 / 329\n",
      "INFO - 2025-06-10 12:37:54,142 - Segment: 151 / 329\n",
      "INFO - 2025-06-10 12:39:08,209 - Segment: 201 / 329\n",
      "INFO - 2025-06-10 12:40:38,609 - Segment: 251 / 329\n",
      "INFO - 2025-06-10 12:41:56,882 - Segment: 301 / 329\n"
     ]
    }
   ],
   "source": [
    "# Torch (pyannote) isn't familiar with MP3 files, so convert to wav for effective performance\n",
    "# Perform diarization to help separate narration in audio description from dialogue in original movie\n",
    "# Finally use OpenAI's Whisper to convert to a transcript\n",
    "\n",
    "mp3_files = [x for x in os.listdir(da.trans_mp3_dir) if os.path.splitext(x)[-1].lower() == '.mp3']\n",
    "\n",
    "for mp3_filename in mp3_files:\n",
    "    movie_name = utils.remove_ext(mp3_filename)\n",
    "    vad_df_path = os.path.join(da.voice_activity_dir, f'{movie_name}-vad.parquet')\n",
    "    seg_df_path = os.path.join(da.diarization_dir, f'{movie_name}-diarization.parquet')\n",
    "    curr_transcript_fp = os.path.join(da.transcription_dir, da.transcript_df_fp.format(movie_name=movie_name))\n",
    "    wav_filepath = os.path.join(da.trans_mp3_dir, f'{movie_name}_speech_only.wav')\n",
    "\n",
    "    # If either diarization or transcript is missing, we'll need to generate the wav file\n",
    "    if not os.path.exists(curr_transcript_fp) or not os.path.exists(seg_df_path):\n",
    "        stt.apply_silero_vad_to_wav(mp3_filename, wav_filepath, vad_df_path, silero_threshold)\n",
    "            \n",
    "    # Only perform diarization if parquet doesn't exist\n",
    "    if not os.path.exists(seg_df_path):\n",
    "        stt.apply_diarization(movie_name, wav_filepath, pyannote_model, seg_df_path, device)\n",
    "        stt.add_pyannote_cosine_sim(seg_df_path, wav_filepath, min_seg_sec=0.3, device=device)\n",
    "\n",
    "    # Only perform transcription if parquet doesn't exist\n",
    "    if not os.path.exists(curr_transcript_fp):\n",
    "        stt.transcribe_segments(curr_transcript_fp, seg_df_path, wav_filepath, whisper_model, whisper_config, narr_cosine_sim_lim, diag_cosine_sim_lim, device)\n",
    "        \n",
    "    # Delete Wav File afterwards as they are quick to generate and consume too much space\n",
    "    if os.path.exists(wav_filepath):\n",
    "        os.remove(wav_filepath)\n",
    "    \n",
    "utils.clean_up_missed_wav_files(da.trans_mp3_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fb1c10",
   "metadata": {},
   "source": [
    "### Subtitles Editted\n",
    "\n",
    "File Completely Empty: X-Men, Finding Neverland, Mr Mrs Smith\n",
    "Grease: Line 6916\n",
    "Hangover Part II: Timestamps messed up line 5578\n",
    "Super Mario Bros. Movie: Line 3877, Missing hours \n",
    "The Social Network: Counter 658, 1507, 1526\n",
    "\n",
    "Index Titles Edited:\n",
    "- Goodbye Columbus\n",
    "- Monsters Inc\n",
    "- What's Up, Doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c826069",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_subs_df_list = []\n",
    "movie_names = set()\n",
    "\n",
    "for movie_cat in ('Blockbusters', 'Oscar'):\n",
    "    cat_mask = films_list_df.fame_category.eq(movie_cat)\n",
    "    for year in films_list_df.year.unique():\n",
    "        year_dir = os.path.join(da.sub_by_year_dir, movie_cat, str(year))\n",
    "        for movie_fp in os.listdir(year_dir):\n",
    "            curr_df = da.extract_single_subs_file(os.path.join(year_dir, movie_fp))\n",
    "            curr_movie = utils.remove_ext(movie_fp)\n",
    "            \n",
    "            # Handle repeat titles like The Little Mermaid by adding the year to subsequent productions\n",
    "            if curr_movie in movie_names:\n",
    "                curr_df['movie'] = curr_movie + f' ({str(year)})'\n",
    "            else:\n",
    "                movie_names.add(curr_movie)\n",
    "            \n",
    "            full_subs_df_list.append(curr_df)\n",
    "                \n",
    "full_subs_df = pd.concat(full_subs_df_list)\n",
    "\n",
    "full_subs_df['movie'] = full_subs_df['movie'].str.strip().str.replace('-', ' ')\n",
    "films_list_df['movie'] = films_list_df['movie'].str.strip().str.replace('-', ' ').str.replace(\"'\", ' ').str.replace('&', 'and')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9635c473",
   "metadata": {},
   "outputs": [],
   "source": [
    "films_list_df.loc[films_list_df.movie.eq('Don t Look Up'), 'movie'] = 'Dont Look Up'\n",
    "films_list_df.loc[films_list_df.movie.eq('Goodbye,Columbus'), 'movie'] = 'Goodbye Columbus'\n",
    "films_list_df.loc[films_list_df.movie.eq('Summer of  42'), 'movie'] = 'Summer of 42'\n",
    "films_list_df.loc[films_list_df.movie.eq('What s Up, Doc_'), 'movie'] = 'What s Up, Doc'\n",
    "films_list_df.loc[films_list_df.movie.eq('Monsters, Inc.'), 'movie'] = 'Monsters Inc'\n",
    "\n",
    "combined_subs_df = full_subs_df.merge(films_list_df, how='left')\n",
    "combined_subs_df.to_parquet(da.sub_df_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "416dbcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_transcripts_df_list = []\n",
    "longitudinal_movies = [utils.remove_ext(x) for x in os.listdir(da.trans_mp3_dir)]\n",
    "\n",
    "for filename in os.listdir(da.transcription_dir):\n",
    "    movie = filename.removesuffix(da.transcript_df_fp.format(movie_name=''))\n",
    "    if movie in longitudinal_movies:\n",
    "        all_transcripts_df_list.append(pd.read_parquet(os.path.join(da.transcription_dir, filename)))\n",
    "\n",
    "all_transcripts_df = pd.concat(all_transcripts_df_list)\n",
    "all_transcripts_df.to_parquet(da.all_transcripts_df_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
